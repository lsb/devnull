\chapter{A Safe Scheduling Language for Structured Parallel Traversals}
\label{chap:3}

\section{Motivation and Approach}
\begin{itemize}
\item structure is good for parallelization
\item parallelization needs checking
\item structured parallelism in layout 
\end{itemize}



\section{Background:  Static Sequential and Task Parallel Visitors}
\subsection{Sequential Visitors}  
\begin{itemize}
\item Knuth: synth and inh
\item OAG
\end{itemize}
\subsection{Task Parallel Visitors} 
\begin{itemize}
\item FNC-2 / Work stealing
\end{itemize}

\section{Structured Parallelism in Visitors} 
\subsection{td, bu, in order} (related to distributed?)
\subsection{concurrent} (old paper: unstructured within visit)
\subsection{multipass} (any old paper? unstructured within visit)
\subsection{nested}

\section{A Behavioral Specification Language}
\subsection{Formalism}

\section{Schedule Compilation}
Phrase as rewrites working in an EDSL w/ templates
\subsection{Rewrite rules}
\section{Schedule Verification}
\subsection{Overview}
\begin{itemize}
\item properties to prove: schedule followed (and complete), dependencies realizable 
\item structure of proof
\end{itemize}
\subsection{Axioms}
\begin{itemize}
\item axioms
\item examples from each
\end{itemize}
\subsection{Proof}

\section{Automatically Staging Memory Allocation for SIMD Rendering}
\input{chap6staging}

\section{Scheduling Loops}


\section{Evaluation: Layout as Structured Parallel Visits}
\subsection{Box model}
\subsection{Nested text}
\subsection{Grids}


\subsection{SIMD Rendering through Staged Memory Allocation}
We evaluate three dimensions of our staged memory allocation approach: flexibility, productivity, and performance. First, it needs to be able to express the rendering tasks that we encounter in GPU data visualization. Second, it should some form of productivity benefit for these tasks. Finally, the performance on those tasks must be fast  enough to support real-time animations and interactions of big data sets.

\subsubsection{Flexibility}
Our staged structuring and automation approach cannot express all dynamic memory usage patterns, so it is important to validate that it works on common patterns that occur in visualization. We found the three following patterns to be important:

\begin{itemize}
\item \textbf{Functional graphics.} Functional graphics primitives used in languages such as Scheme, O'Caml, and Haskell follow the form that we used for \code{Circle}. For example, many of our visualizations use simple variants such as 2D rectangles,  3D line, and arcs.
\item \textbf{Linked view}. Multiple renderable objects can be associated with one node, which we can use for providing different views of the same data. Such functionality is common for statistical analysis software:

\begin{lstlisting}[mathescape]
render :=  @Circle(x,y,r)  + @Circle(offsetX + abs(x), offsetY + abs(y), r);
\end{lstlisting}

\item \textbf{Zooming.} We can use the same multiple representation capability for a live zoomed out view (``picture-in-picture''):

\begin{lstlisting}[mathescape]
render :=  
  @Circle(x, y, radius) 
   + @Circle(xFrame + x*zoom, yFrame + y*zoom, radius *zoom);
\end{lstlisting}

\item \textbf{Visibility toggles.} Our macros support conditional expressions, which enables controlling whether to render an object. For example, a boolean input attribute can control whether to show a circle: \code{render := isOn ? @Circle(0,0,10) : 0; }
\item \textbf{Alternative representations.} Conditional expressions also enable choosing between multiple representations, not just on/off visibility:
\begin{lstlisting}[mathescape]
render := 
  isOff ? 0
    : mouseHover ? @CircleOutline(0,0,10) 
    : @Circle(0,0,10,5) ;
\end{lstlisting}

\end{itemize}

\subsubsection{Productivity}
Productivity is difficult to measure. Before using the automation extensions for rendering, we repeatedly encountered bugs in manipulating the allocation calls and memory buffers. The bugs related both to incorrect scheduling and to incorrect pointer arithmetic. Our new design eliminates the possibility of both bugs.

One suggestive productivity measure is of how many lines of code the macro abstraction eliminates from our visualizations. We measured the impact on using it for 3 of our visualizations. The first visualization is our HBox language extended with rendering calls, while the other two are interactive reimplementations of popular visualizations: a treemap~[[CITE]] and multiple 3D line graphs~[[CITE]].


\begin{table}[ht]
\caption{Lines of Code Before/After Invoking the '@' Macro}
\centering
\begin{tabular}{c r r r}
\hline\hline
 \textbf{Visualization} & \textbf{Before (loc)} & \textbf{After (loc)} & \textbf{Decrease} \\ [0.5ex] \hline
  HBox & 97 & 54 & 44\% \\
  Treemap & 296 & 241 & 19\% \\
  GE & 337 & 269 & 20\% \\ [1ex] 
\hline
\end{tabular}
\label{table:macroreduction}
\end{table}
Table~\ref{table:macroreduction} compares the lines of code in visualizations before and after we added the macros. Using the macros eliminated 19--44\% of the code. Note that we are \emph{not} measuring the macro-expanded code, but code that a human wrote.



As shown in Figure~\ref{fig:stagedallocClient}, the eliminated code is code that was introduced by staging the library calls. Porting unstaged functional graphics calls to the library, is in practice, an alpha renaming of function names.  Using the '@' macro eliminates 19--44\% of the code that would have otherwise been introduced and completely eliminates two classes of bugs (scheduling and pointer arithmetic), so the productivity benefit is non-trivial. 

\subsubsection{Performance}


\section{Related Work}
Lang of schedules
\begin{itemize}
\item background
\item stencils and skeletons: wavefront, ...
\item polyhedra
\end{itemize}
Schedule verification
\begin{itemize}
\item compare to OAG etc., looser dataflow/functional langs
\end{itemize}