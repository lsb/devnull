\chapter{Optimizing Tree Traversals for MIMD and SIMD}
\section{Overview}

For a full language, statically identified parallelization opportunities still require an efficient runtime implementation that exploits them. In this section, we show how to exploit the logical concurrency identified within a tree traversal to optimize for the architectural properties of two types of hardware platforms: MIMD (e.g., multicore) and SIMD (e.g., sub-word SIMD and GPU) hardware. For both types of platforms, we optimize the schedule within a traversal and the data representation. We innovate upon known techniques in two ways:

\begin{enumerate}
\item \textbf{Semi-static work stealing for MIMD:} MIMD traversals should be optimized for low overheads, load balancing, and locality. Existing techniques such as work stealing provide spatial locality and, with tiling, low overheads. However, dynamic load balancing within a traversal leads to poor temporal locality across traversals. The processor a node is assigned to in one traversal may not be the same one in a subsequent traversal, and as the number of processors increases, the probability of assigning to a different one increases. Our solution dynamically load balances one traversal and, due to similarities across traversals, successfully reuses it.
\item \textbf{Clustering traversals for SIMD:}  SIMD evaluation is sensitive to divergence across parallel tasks in instruction selection. Visits to different types of tree nodes yield different instruction streams, so na\i{v}e vectorization fails for webpages due to their visual variety. Our insight is that similar nodes can be semi-statically identified. Thus \emph{clustered} nodes will be grouped in the data representation and run in SIMD at runtime.
%\item \textbf{Automatically staged parallel memory allocation to efficiently combine SIMD layout and SIMD rendering:} We optimized the schedule of memory allocations in the layout computation into an efficient parallel prefix sum. Otherwise, parallel dynamic memory allocation requests would contend over the free memory buffer and void GPU performance benefits. We automated the optimization by reducing the scheduling problem to attribute grammar scheduling and automatically performing the reduction through macro expansion.
\end{enumerate}

Our techniques are important and general. They overcame bottlenecks preventing seeing any speedup from parallel evaluation for webpage layout and data visualization. Notably, they are generic to computations over trees, not just layout. An important question going forward is how to combine them as, in principle, they are complementary.

\section{MIMD: Semi-static work stealing}
\input{chap6mimd}


\section{SIMD Background: Level-Synchronous Breadth-First Tree Traversal}
The common baseline for our two SIMD optimizations is to implement parallel preorder and postorder tree traversals as level-synchronous breadth-first parallel tree traversals. Reps first suggested such an approach to parallel attribute grammar evaluation [[CITE]], but did not implement it. Performance bottlenecks led to us deviate from the core representation used by more recent data parallel languages such as NESL [[CITE]] and Data Parallel Haskell [[CITE]]. We discuss our two innovations in the next subsections, but first overview the baseline technique established by existing work.

\newsavebox{\bfsVisitor}
\begin{lrbox}{\bfsVisitor}% Store first listing
\begin{lstlisting}[mathescape,language=C++,morekeywords={spawn,join,reverse,parallel_for}]
void parPre(void (*visit)(Prod &), List<List<Prod>>  &levels) {
  for (List<Prod> level in levels)
  	parallel_for (Prod p in level)
		visit(p)
}
void parPost(void (*visit)(Prod &), List<List<Prod>>  &levels) {
  for (Array<Prod> level in levels.reverse())
  	parallel_for (Prod p in level)
		visit(p)
}\end{lstlisting}
\end{lrbox}

\begin{figure}
\subfloat[\textbf{Level-synchronous Breadth-First Traversal}]{\label{fig:bfstraversal:code}  \usebox{\bfsVisitor}  } \\
\subfloat[\textbf{Logical Tree}]{\label{fig:bfstraversal:replog} \includegraphics[trim=0 0 0 0,clip,width=0.3\columnwidth]{chapter6/bfslayouttree} } 
 \subfloat[\textbf{Tree Representation}]{\label{fig:bfstraversal:repmem}  \includegraphics[trim=0 0 0 0,clip,width=0.6\columnwidth]{chapter6/bfslayoutmem} } 
\caption{\textbf{SIMD tree traversal as level-synchronous breadth-first iteration with corresponding structure-split data representation.}}
\label{fig:bfstraversal}
\end{figure}

The na\i{v}e tree traversal schedule is to sequentially iterate one level of the tree at a time and  traverse the nodes of a level in parallel. A parallel preorder traversal starts on the root node's level and then proceeds downwards, while a postorder traversal starts on the tree fringe and moves upwards (Figure~\ref{fig:bfstraversal}~\ref{fig:bfstraversal:code}). Our MIMD implementation, in contrast, allows one processor to compute on a different tree level than another active processor. In data visualizations, we empirically observed that most of the nodes on a level will dispatch to the same layout instructions, so our na\i{v}e traversal schedule avoids instruction divergence.

The level-synchronous traversal pattern eliminates many divergent memory accesses by using a corresponding data representation. Adjacent nodes in the schedule are collocated in in memory. Furthermore, individual node attributes are stored in \emph{column} order through a array-of-structure to structure-of-array conversion. The conversion collocates individual attributes, such as the width attribute of one node being stored next to the width attribute of the node's sibling (Figure~\ref{fig:bfstraversal:repmem}). The index of a node in a breadth-first traversal of the tree is used to perform a lookup in any of the attribute arrays. The benefit this encoding is that, during SIMD  layout of several adjacent nodes, reads and writes are coalesced into  bulk reads and writes. For example, if a layout pass adds a node's padding to its width, several contiguous paddings and several contiguous widths will be read, and the sum will be stored with a contiguous write. Such optimizations are crucial because the penalty of non-coalesced access is high and, for layout, relatively few computations occur between the reads and writes.

Full implementation of the data representation poses several subtleties. 
\begin{itemize}
\item \textbf{Level representation.} To eliminate traversal overhead, a summary provides the index of the first and last node on each level of a tree. Such a summary provides  data range information for launching the parallel kernels that evaluate the nodes of a level as well as the information for how to proceed to the next level.
\item \textbf{Edge representation.} A node may need multiple named lists of children, such as an HTML table with a header, footer, and an arbitrary number of rows. We encode the table's edges as 3 global arrays of offsets: header, footer, and first-row. To support iterating across rows, we also introduce a 4th array to encode whether a node is the last sibling. Thus, any named edge introduces a global array for the offset of the pointed-to node, and for iteration, a shared global array reporting whether a node at a particular index is the end of a list.
\item \textbf{Memory compression.} Allocating an array the size of the tree for every type of node attribute wastes memory. We instead statically compute the maximum number of attributes required for any type of node, allocate an array for each one, and map the attributes of different types of nodes into different arrays. For example, in a language of HBox nodes as Circle nodes who have attributes 'r' and 'angle', 4 arrays will be allocated. The HBox requires an array for each of the attributes 'w', 'h', 'x', and 'y' while the Circle nodes only require two arrays. Each node has one type, and if that that type is HBox, the node's entry in the first array will contain the 'w' attribute. If the node has type Circle, the node's entry in the first entry will contain the 'r' attribute.
\item \textbf{Tiling.} Local structural mutations to a tree such as adding or removing nodes should not force global modifications. As most SIMD hardware has limited vector lengths (e.g., 32 elements wide), we split our representation into blocks. Adding nodes may require allocation of a new block and reorganization of the old and new block. Likewise, after successive additions or deletions, the overall structure may need to be compacted. Such techniques are standard for file systems, garbage collectors, and databases.
\end{itemize}


In summary, our basic SIMD tree traversal schedule and data representation descend from the approach of NESL [[CITE]] and Data Parallel Haskell [[CITE]]. Previous work shows how to generically convert a tree of structures into a structure of arrays. Those approaches do not support statically unbounded nesting depth (i.e., tree depth), but our system supports arbitrary tree depth because our transformation is not as generic.  

A key property of all of our systems, however, is that the structure of the tree is fixed prior to the traversals.  In contrast, for example, parallel breadth-first traversals of graphs will dynamically find a minimum spanning tree [[CITE]]. Such dynamic alternatives incur unnecessary overheads when performing a sequence of traversals and sacrifice memory coalescing opportunities. Layout is often a repetitive process, whether due to multiple tree traversals for one invocation or an animation incurring multiple invocations, so costs in creating an optimized data representation and schedule are worth paying.

\section{Input-dependent Clustering for SIMD Evaluation}
\input{chap6simd}


\section{Evaluation}






\begin{table}[t]
\center
\begin{tabular}{l|c|c|c|c| c | c | c}
~  &\multicolumn{4}{c|}{\textbf{Total speedup}} & \multicolumn{3}{c}{\textbf{Parallel speedup}}\\
~  &\multicolumn{4}{c|}{Cores} & \multicolumn{3}{c}{Cores}\\
{\small \textbf{Configuration}} & 1 & 2 & 4 & 8 & 2 & 4 & 8\\ [0.5ex] \hline \hline
TBB, \code{server} & 1.2x & 0.6x & 0.6x & 1.2x & 0.5x & 0.5x & 1.0x \\ \hline
FTL, \code{server} & 1.4x & 2.4x & 5.2x & 9.3x & 1.8x & 3.8x & 6.9x \\ 
FTL, \code{laptop} & 1.4x & 2.1x & & & 1.6x &     &  \\ 
FTL, \code{mobile} & 1.3x & 2.2x & & & 1.7x &     &  \\ 
\end{tabular}
\caption{\textbf{Speedups and strong scaling across different backends (Back) and hardware}. Baseline is a sequential traversal with no data layout optimizations. FTL is our multicore tree traversal library. Left columns show total speedup (including data layout optimizations by our code generator) and right columns show just parallel speedup. Server = Opteron 2356, laptop = Intel Core i7, mobile = Atom 330.}
\label{tab:diffhw}
\end{table}

\subsection{MIMD Data Representation and Scheduling Optimizations}
By statically exposing traversal structure (e.g., \sched{parPre}) to our code generators, we observe sequential and parallel speedups. We separately evaluate the importance of the data representation optimizations from the scheduling ones on random 500-1000 node documents in the \hlangpp{} language. Finally, we examine the parallel benefit on webpages.


We first evaluate the perform of our task scheduler (FTL in Table~\ref{tab:diffhw}).  Our comparison point is Intel's TBB~\cite{inteltbb} dynamic task scheduler that performs work stealing~\cite{cilk}, which was the most efficient third-party work stealing library that we tried. We included our data layout optimizations in all calculations because, without them, we saw no speedup.  TBB causes slowdowns until achieving no cost (nor benefit) at 8 cores. Our insight is that it suffered from high overheads: switching to scheduling tiles by using our optimized data representation improved performance. Our semi-static working stealing scheduler, however, achieved a 6.9X speedup on 8 cores. We did not see significant further speedups for higher core counts, and hypothesize that it is due to the socket jump. We experimented with other schedulers, such as a simple for-loop over tiles near the fringe of the tree, but the achieved 2X speedup is much lower than the 6.9X of our semi-static work stealer.


Data representation was key to achieving parallel speedups. It achieved 1.2X-1.4X speedups for sequential processing (Table~\ref{tab:diffhw}). However, on 4 cores, it improved performance from 2.8X without data representation optimizations to 5.2X when using them. The difference is 1.9X: our data representation optimizations both complement and improve scheduling optimizations. Without them, parallel performance was poor.

\begin{table}[t]
\center
\begin{tabular}{l|l| c | c | c}
~ & & \multicolumn{3}{c}{\textbf{Parallel speedup}}\\
~ & & \multicolumn{3}{c}{Cores}\\
\textbf{Backend} & Input & 2 & 4 & 8\\ [0.5ex] \hline \hline
TBB & {Wikipedia} &  1.5x & 1.6x & 1.2x \\
TBB & {xkcd Blog} &  1.5x & 1.8x & 1.2x \\ \hline
FTL & {Wikipedia} &  1.6x & 2.8x & 3.2x \\
FTL & {xkcd Blog} &  1.5x & 2.3x & 3.1x \\ 
\end{tabular}
\caption{\textbf{Parallel CSS layout engine}. Run on a 2356 Opteron.}
\label{fig:cssperf}
\end{table}

Table~\ref{fig:cssperf} shows the parallel speedup on running our 9 pass layout engine for two popular web pages that render faithfully with it: Wikipedia and the XKCD blog. Note that the benchmarks do \emph{not} include sequential speedups. The best performance of TBB was a 1.8X speedup on 4 cores, and its speedup on 8 cores was 1.2X. In comparison, our scheduler achieved 2.8X on 2 cores and 3.2X on 8X. Our insight as to why we did not see further benefits is overheads. Across our benchmarks, we generally saw speedups when sequential traversals took longer than a certain amount, but because so many traversals are used for CSS, enough of them are small enough that we do not expect strong scaling. Our intuition is that either a full layout engine is complicated enough that the sequential cost of each traversal will be higher than in our prototype, or even more aggressive data representation optimizations should be performed. As is, we have demonstrated significant 3X+ speedups on real workloads from just the parallelization.


\begin{figure}
\centering
\includegraphics[trim=0 0 0 0,clip,width=1.0\columnwidth]{chapter6/gpuspeedup}
\caption{\textbf{Sequential and Parallel Benefits of Breadth-First Layout and Staged Allocation.} Allocation is merged into the 4th stage and buffer indexing and tessellation form the rendering pass. JavaScript variants use HTML5 canvas drawing primitives while WebCL does not include WebGL painting time ($<$ 5ms). Thin vertical bars indicate standard deviation and horizontal bars show deadlines for animation and hand-eye interaction.}
\label{fig:treemapjsgpu}
\end{figure}


\subsection{Baseline SIMD Speedups (GPU)}
We evaluate the sequential and parallel performance benefits of our baseline breadth-first layout.   For an animation to achieve 24fps, the time spent to process a frame should not exceed 42ms, and for eye-hand interactions, 100ms (10fps). We examine the case of a 5 pass treemap that supports live filtering over 100,000 data points. The first 3 passes are purely devoted to layout, the 4th pass includes layout computations and allocation requests, and the 5th pass propagates buffer indices and performs tessellation. 

We compare 3 backends for our compiler: canonical JavaScript (a tree of nodes), JavaScript over our structure-split breadth-first tree layout (and with typed arrays~[[CITE]]), and WebCL for the GPU.  The first two variants invoke HTML5 canvas drawing primitives, while the last invokes WebGL (GPU) painting primitives over vertex buffers computed in the rendering pass. The time for WebGL painting calls are not shown, but they take less than 5ms. Each variant is repeated 15 times on a 4 core 2012 2.66GHz Intel Core i7 with 8 GB memory and a 1024 MB NVIDIA GeForce GT 650M graphics card.

We first examine  the significant sequential benefits.The first 4 groups of columns in Figure~\ref{fig:treemapjsgpu} shows the average time spent on different layout passes and the 6th on the pass for buffer index computation and tessellation. Performing compiler optimizations enables a 14X sequential speedup on layout in the Chrome web browser. No speedup is observed in the rendering pass because the time is dominated by HTML5 canvas calls. We hypothesize part of the sequential benefit is related to our clustering optimization: all of the nodes in a level have the same type, so implicit optimizations such as branch prediction should perform better. Finally, we note that while sequential layout time is a magnitude too slow for real-time animation, our prototype is within 54ms for real-time interaction (ignoring rendering).

Parallel speedups are also significant. WebCL (GPU) evaluation of layout is 5X faster than sequential. The impact of compiling JavaScript vs. C (WebCL) on the benchmark is unclear: JavaScript is generally a magnitude slower than native code, except the runtime WebCL compiler is not running at high optimization levels. The benefits for parallel computation of the buffer indices and tessellation is much more clear: the speedup is 31X. 

\begin{figure}
\centering
\includegraphics[trim=0 0 0 0,clip,width=1.0\columnwidth]{chapter6/gpuvscpu}
\caption{\textbf{Multicore vs. GPU Acceleration of Layout.} Benchmark on an early version of the treemap visualization and does not include rendering pass.}
\label{fig:cpuvsgpu}
\end{figure}

To better understand the benefit of parallelization, we compared running the layout traversals using multicore vs. GPU acceleration (Figure~\ref{fig:cpuvsgpu}) for an early prototype of the layout traversals. Both use breadth-first traversals compiled with OpenCL, except differ on the hardware target. We see that a server-grade multiprocessor (32-core AMD Opteron 2356) can outperform a laptop GPU, but the comparison is unfair in terms of power consumption. TODO compare power ratings.


Ultimately, when the sequential and parallel optimizations are combined, we see an end-to-end speedup of 54X. It is high enough such that it enables real-time animation for our data set, not just real-time user interaction.


\subsection{SIMD Clustering}
We evaluate several aspects of our clustering approach. First, we examine applicability to various visualizations. Second, we evaluate the speed and performance benefit. Clustering provides invariants that benefit more than just vectorization, so we distinguish sequential vs. parallel speedups. Finally, there are different options in what clusters to form, so for each stage of evaluation, we compare impact.

\begin{figure}
\centering
\includegraphics[trim=0 0 0 0,clip,width=1.0\columnwidth]{chapter6/csscompression}
\caption{\textbf{Compression ratio for different CSS clusterings.} Bars depict compression ratio (number of clusters over number of nodes). Recursive clustering is for the reduce pattern, level-only for the map pattern. ID is an identifier set by the C3 browser for nodes sharing the same style parse information while value is by clustering on actual style field values.}
\label{fig:csscompression}
\end{figure}




\subsubsection{Applicability}


We examined idealized speedup for several workloads:

\begin{itemize}

\item \textbf{Synthetic.} For a controlled synthetic benchmark, we simulated the effect of increasing number of clusters on speedup for various SIMD architectures.  Our simulation assumes perfect speedups for SIMD evaluation of nodes run together on a SIMD unit. The ideal speedup is a function of the minimum of the SIMD unit's length (for longer clusters, multiple SIMD invocations are mandatory) and the number of clusters (at least one SIMD step is necessary for each cluster).   Figure~\ref{fig:simulatedclusteringspeedup} shows, for architectures of different vector length, that the simulated speedup from clustering (solid black line with circles) is close to the ideal speedup (solid green line).

\item \textbf{Data visualization.} For our data visualizations, we found that, across the board, all of the nodes of a level shared the same type. For example, our visualization for multiple line graphs puts the root node on the first level, the axis for each line graph on the second level, and all of the actual line segments on the third level. 

\item \textbf{CSS.} We analyzed potential speedup on webpages. Webpages are a challenging case because an individual webpage features high visual diversity, with popular sites using an average of 27KB of style data per page.~\footnote{https://developers.google.com/speed/articles/web-metrics}. We picked 10 popular websites from the Alexa Top 100 US websites that rendered sufficiently correctly in the C3~[[CITE]] web browser. It was also challenging in practice because it required clustering based on individual node attributes, not just the node type.

Figure~{fig:csscompression} compares how well nodes of a webpage can be clustered. It reports the \emph{compression ratio}, which divides the number of clusters by the number of nodes. Sequential execution would assign each node to its own cluster, so the ratio would be 1. In contrast, if the tree is actually a list of 100 elements, and the list can be split into 25 clusters, the ratio would be 25\%. Assuming infinite-length vector processors and constant-time evaluation of a node, the compression ratio is the exact inverse of the speedup. A ratio of 1 leads to a 1X speedup, and a compression ratio of 25\% leads to a 4X speedup.

Clustering each level by attributes that influence control flow achieved a 12\% compression ratio (Figure~{fig:csscompression}): an 8.3X idealized speedup. When we strengthened the clustering condition to enforce stronger invariants in the cluster, such as to consider properties of the parent node, the ratio quickly worsened. Thus, we see that our basic approach is promising for websites on modern subword-SIMD instruction sets, such as a 4-wide SSE (x86) and NEON (ARM), and the more recent 8-wide AVX (x86). Even longer vector lengths are still beneficial because some clusters were long. However, eliminating all divergences requires addressing control flows influenced by attributes of node neighbors, which leads to poor compression ratios. Thus, we emphasize that 8.3X is an upper bound on the idealized speedup: not all branches in a cluster are addressed.
\end{itemize}

Empirically, we see that clustering is applicable to CSS, and in the case of our data visualizations, unnecessary. Vectorization limit studies based on analyzing dynamic data dependencies from program traces suggest that general programs can be much more aggressively vectorized, so clustering may be the beginning of one such approach~[[CITE]].




\begin{figure}
\centering
\includegraphics[trim=0 0 0 0,clip,width=1.0\columnwidth]{chapter6/cssspeedup4}
B =breadth first, S = structure splitting, M = level clustering, R = nested clustering, H = hoisting, V = SSE 4.2 
\caption{\textbf{Speedups from clustering on webpage layout.} Run on a 2.66GHz Intel Core i7 (GCC 4.5.3 with flags -O3 -combine -msse4.2) and does not preprocessing time.
}
\label{fig:cssspeedup}
\end{figure}

\subsubsection{Speedup}
We evaluate the speedup benefits of clustering for webpage layout. We take care to distinguish sequential benefits from parallel, and of different clustering approaches. Our implementation was manual:  we examine optimizing one pass of the C3~[[CITE]] browser's CSS layout engine that is responsible for computing intrinsic dimensions. The C3 browser was written in C\#, so we wrote our optimized traversal in C\+\+ and pinned the memory for shared access.  We use a breadth-first tree representation and schedule for our baseline, but note that doing such a layout already provides a speedup over C3's unoptimized global layout. 

For our experimental setup, we evaluate the same popular webpages above that rendered legibly with the experimental C3 browser.  Benchmarks ran on a 2.66GHz Intel Core i7 (GCC 4.5.3 with flags -O3 -combine -msse4.2). We performed 1,000 trials, and to avoid warm data cache effects, iterated through different webpages.

We first examine sequential performance. Converting an array-of-structures to a structure-of-arrays causes a 10\% slowdown (B S in Figure~\ref{fig:cssspeedup}). However, clustering each level and hoisting computations shared throughout a cluster led to a 2.1X sequential benefit (M S H). Nested clustering provided more optimization opportunities, but the compression ratio worsened: it only achieved a 1.7X sequential speedup (R S H). Clustering provides a significant sequential speedup.

Next, we examine the benefit of vectorization. SSE instructions provide 4-way SIMD parallelism. Vectorizing the nested clustering improves the speedup from 1.7X to 2.6X, and the level clustering from 2.1X to 3.5X. Thus, we see significant total speedups. The 1.7X relative speedup of vectorization, however, is still far from the 4X: level clustering suffers from randomly strided children, and the solution of nested clustering sacrifices the compression ratio.

\begin{figure}
\centering
\includegraphics[trim=0 0 0 0,clip,width=1.0\columnwidth]{chapter6/csspower}
\caption{\textbf{Performance/Watt increase for clustered webpage layout.}}
\label{fig:csspower}
\end{figure}

\subsubsection{Power}
Much of our motivation for parallelization is better performance-per-Watt, so we evaluate power efficiency. To measure power, we sampled the power performance counters during layout. Each measurement looped over the same webpage over 1s due to the low resolution of the counter. Our setup introduces warm cache effects, but we argue it is still reasonable because a full layout engine would use multiple passes and therefore also have a warm cache across traversals.

In Figure~\ref{fig:csspower}, we show a 2.1X improvement in power efficiency for clustered sequential evaluation, which matches the 2.1X sequential speedup of Figure~\ref{fig:cssspeedup}. Likewise, we report a 3.6X cumulative improvement in power efficiency when vectorization is included, which is close to the 3.5X speedup. Thus, both in sequential and parallel contexts, clustering improves performance per Watt. Furthermore, it supports the general reasoning in parallel computing of 'race-to-halt' as a strategy for improving power efficiency.


\begin{figure}
\centering
\includegraphics[trim=0 0 0 0,clip,width=0.6\columnwidth]{chapter6/datarelayouttime3}
\caption{\textbf{Impact of data relayout time on total CSS speedup.} Bars depict layout pass times. Speedup lines show the impact of including clustering preprocessing time.}
\label{fig:cssrelayout}
\end{figure}



\subsubsection{Overhead}
Our final examination of clustering is of the overhead. Time spent clustering before layout must not outweigh the performance benefit; it is an instance of the planning problem. 

For the case of data visualization, we convert the data structure into arrays with an offline preprocessor. Thus, our data visualizations experience no clustering cost.

For webpage layout, clustering is performed on the client when the webpage is received. We measured performing sequential two-pass clustering. Figure~\ref{fig:cssrelayout} shows the overhead relative to one pass using the bars. The highest relative overhead was for the Flickr homepage, where it reaches almost half the time of one pass. However, layout occurs in multiple passes. For a 5-pass layout engine where we model each pass as similar to the one we optimized, the overhead is amortized. The small gap between the solid and dashed lines in Figure~\ref{fig:cssrelayout} show there is little difference when we include the preprocessing overhead in the speedup calculation.



\section{Related Work}
\begin{enumerate}
\item representation The representation might be further compacted. For example, the last two arrays will have null values for Circle nodes. Even in the case of full utilization, space can be traded for time for even more aggressive compression [[CITE rinard]]
\item sims limit studies
\item duane
\item trishul
\item gnu irregular array stuff
\end{enumerate}
